from langchain_core.language_models import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import Optional, List, Dict, Any, AsyncIterator
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList
import torch

class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_token_ids:List[int]):
        self.stop_token_ids=stop_token_ids

    def __call__(self, input_ids:torch.LongTensor, scores:torch.FloatTensor, **kwargs) ->bool :
        last_token = input_ids[0,-1].item()
        
        return last_token in self.stop_token_ids

class CustomHuggingFaceLLM(LLM):
    model : Optional[Any]=None
    tokenizer : Optional[Any]=None
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    def __init__(self, model_path:str, **kwargs):
        super().__init__(**kwargs)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if self.device=="cuda" else torch.float32,
            device_map="auto"
            )
        self.model.eval()

        # Explicitly set pad_token to avoid using eos_token_id as padding
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id=self.tokenizer.eos_token_id
            self.tokenizer.pad_token=self.tokenizer.eos_token
    
    @property
    def _llm_type(self) -> str :
        return "custom_huggingface_llm"
    
    def _call(
        self,
        prompt:str,
        stop:Optional[List[str]]=None,
        run_manager:Optional[CallbackManagerForLLMRun]=None,
        **kwargs
        ) -> str:
        
        inputs=self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, return_attention_mask=True).to(self.device)
        input_length=inputs["input_ids"].shape[1]
        generate_kwargs={
            # "max_length":kwargs.get("max_length",512),
            "max_new_tokens":kwargs.get("max_new_tokens",512),
            "num_return_sequences":1,
            "do_sample":True,
            "top_k":50,
            "top_p":0.95,
            "temperature":kwargs.get("temperature",0.9),
            "pad_token_id":self.tokenizer.pad_token_id,
            "eos_token_id":self.tokenizer.eos_token_id,
            "no_repeat_ngram_size":2,
            "min_length":1,
        }

        generate_kwargs.update(kwargs)

        stopping_criteria=None
        if stop:
            stop_token_ids=[]
            for stop_token in stop:
                token_ids = self.tokenizer.encode(stop_token, add_special_tokens=False)
                if token_ids:
                    stop_token_ids.extend(token_ids)
            if stop_token_ids:
                stopping_criteria=StoppingCriteriaList([StopOnTokens(stop_token_ids)])
                generate_kwargs["stopping_criteria"]=stopping_criteria

        try:
            outputs = self.model.generate(
                inputs["input_ids"],
                attention_mask=inputs.get("attention_mask"),
                **generate_kwargs
            )

            generated_tokens=outputs[0,input_length:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            if stop and not stopping_criteria:
                for stop_token in stop:
                    generated_text=generated_text.split(stop_token)[0]
            generated_text=generated_text.replace("assistant","").strip()
            return generated_text

        except:
            return ""   
    
    async def _astream(
        self,
        prompt:str,
        stop:Optional[List[str]]=None,
        run_manager:Optional[CallbackManagerForLLMRun]=None,
        **kwargs
    )-> AsyncIterator[str] :

        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(self.device)
        input_length = inputs["input_ids"].shape[1]

        generate_kwargs={
            # "max_length":kwargs.get("max_length",512),
            "max_new_tokens":kwargs.get("max_new_tokens",512),
            "do_sample":True,
            "top_k":50,
            "top_p":0.95,
            "temperature":0.9,
            "pad_token_id":self.tokenizer.pad_token_id,
            "no_repeat_ngram_size":2,
            "min_length":1,
        }

        generate_kwargs.update(kwargs)

        for token in self.model.generate(
            inputs["input_ids"],
            attention_mask=inputs.get("attention_mask"),
            **generate_kwargs
        ):
            generated_tokens = token[input_length: ]
            chunk=self.tokenizer.decode(genrated_tokens, skip_special_tokens=True)
            chunk = chunk.replace("assistant","").strip()
            if stop and any(stop_token in chunk for stop_token in stop):
                break
            if run_manager:
                await run_manager.on_llm_new_token(chunk)
            yield chunk

    @property
    def _identyfying_params(self) -> Dict[str,Any]:
        return {
            "model_path":self.model.config._name_or_path if self.model else "unknown",
            "device":self.device,
        }


# if __name__=="__main__":
#     model_path="/exacto/abhishek/responsible_ai/models"
#     custom_llm=CustomHuggingFaceLLM(model_path=model_path)
#     context= "Inida a very kind and good country with no any racism and it is consider as technological peak of the world with fifth largest economy."
#     question="What is capital of India?"
#     prompt = (
#         f"You are a helpful asssistant that provide specific answer only in very very minimal words strictly based on the provided context, without using any external knowledge.\n"
#         f"Given the following Context:{context}\n"
#         f"Answer the Question:{question}\n"
#     )
#     print(prompt)
#     response_with_params=custom_llm.invoke(
#         prompt,
#         max_new_tokens=50,
#         temperature=0.3,
#         stop=["\n"]
#     )
#     print("response with extra   ", type(response_with_params))
#     print(response_with_params)
